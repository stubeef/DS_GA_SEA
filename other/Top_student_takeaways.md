## The main takeaways as expressed by students the last day of class

**matty325**
* Takeaways:
 * Competence at gathering data
 * tools to survey data
 * familiarity with a range of modeling methods
 * how to apply and evaluate results
 * discipline to think about how to iteratively solve problems at scale
 * Everything comes down to bias-variance tradeoff
* Tidbits:
 * Lots of code examples
 * Now I know where a lot of good online resources
 * It was good to learn how to use python notebooks
 * also good to familiarize with github
 * command line refresher was also helpful
* Surprises: There are a goddamn lot of different ways to look at data, but no matter how fancy, everything comes down to maximize gain/minimize loss

**rhosogi**
* Top Takeaways:
 * Core use of pandas in Python; this was something I actually tried to learn before classes and it wasn't as instinctual
 * Using estimators 
 * Reviewing different types of predictors based on datasets
 * Clustering was pretty awesome
 * Basic understanding of a real coding language
* Tidbits:
 * StackOverflow is amazing
 * The devil is in the detail (syntax, random Internet findings)
 * Never feel like an idiot for taking time to prep your data
 * Clustering is fun in theory and practice
 * Kaggle seems like it could become an addiction
* Surprises:
 * Predictive modeling (surprising, not really a surprise)
 * Python was NOT as hard as everyone says it was
 * Any and all industry knowledge dropped in class
 * Data science is surprisingly vast as an industry but this is mostly awesome
 * You'd be surprised by how much stats you remember after 10 years

**apm**
* Top Takeaways:
  * Cross validation.
  * NLP -> TF-IDF, stemming
  * Ensembling -> bagging, boosting
* Tibits (things I want to do more..)
  * Visualize performance
  * Piping
* Surprises
  * How ‘simple’ many models are..
  * Neural networks are simple to construct, but hard to understand.

**bruceakerseattle**
* Takeaways
  * JSON
  * Web API
  * bias vs variance
  * cross validation
* Tidbits
  * Slack
* Surprises
  * SciKitLearn has everything

**ftseng**
* Top takeaways:
  * For machine learning, fewer features may be better (especially when you have thousands)
  * Syntax is not necessarily consistent across python tools like pandas or matplotlib, so there’s nothing wrong with looking up commands and cutting and pasting.
  * I need to find a way to organize my coding better. I've carried over to Python bad habits from SAS and Stata
  * Smart machine learning models take advantage of averaging over many models
* Top surprises:
  * the accuracy at which the kaggle participants predict is astounding
  * per Bruce's project, did not know there was a way one could code and reach images

**katylinn**
* Top takeaways:
  * Copy/Pasting (once you understand) is the only practical way to go, so keep a “cookbook” going
  * Models aren’t “good” or “bad” till you’ve tried tuning
  * I’m not doing anything special… if i’m having trouble, some one else has already asked on stack overflow… just google
  * Anatomy of the Kaggle Competition site
  * Conversational understanding of the commonly discussed models - naive bayes, neural network, random forest  
* Top surprises:
  * I like data gathering and cleaning (i knew that already), but wasn’t very motivated in the Modelling and tuning
  * Naive bayes assumes independence, but is fairly robust against it
  * Picking a model based on your data set is not intuitive… you gotta try them all, tune them all, and probably combine
